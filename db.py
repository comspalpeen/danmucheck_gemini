# db.py
import time
import asyncio
import json
from datetime import datetime
import logging
from motor.motor_asyncio import AsyncIOMotorClient
from pymongo.errors import PyMongoError, BulkWriteError, CollectionInvalid
from pymongo import IndexModel, ASCENDING, DESCENDING
from redis_client import get_redis

logger = logging.getLogger("DB")


def datetime_serializer(obj):
    """JSON åºåˆ—åŒ–æ—¶å¤„ç† datetime å¯¹è±¡"""
    if isinstance(obj, datetime):
        return obj.isoformat()
    raise TypeError(f"Type {type(obj)} not serializable")


def datetime_deserializer(data: dict) -> dict:
    """å°† ISO æ ¼å¼å­—ç¬¦ä¸²è¿˜åŸä¸º datetime å¯¹è±¡"""
    if 'created_at' in data and isinstance(data['created_at'], str):
        try:
            data['created_at'] = datetime.fromisoformat(data['created_at'])
        except ValueError:
            data['created_at'] = datetime.now()
    return data


class AsyncMongoDBHandler:
    def __init__(self, uri="mongodb://gogogo:chufale@localhost:4396/admin", db_name="douyin_live_data"):
        try:
            # Motor çš„è¿æ¥å»ºç«‹æ˜¯éé˜»å¡çš„
            self.client = AsyncIOMotorClient(uri, serverSelectionTimeoutMS=5000)
            self.db = self.client[db_name]
            
            # Redis ç¼“å†²åŒº Key
            self.REDIS_CHAT_KEY = "buffer:chats"
            self.REDIS_GIFT_KEY = "buffer:gifts"
            self.REDIS_STATS_KEY = "buffer:stats"  # ã€æ–°å¢ã€‘ç»Ÿè®¡æ•°æ®ç¼“å†²Key            
            # é…ç½®
            self.BATCH_SIZE = 500
            self.LAST_WRITE_TIME = time.time()
            self.BUFFER_TIMEOUT = 5  # ç¼©çŸ­å†™å…¥é—´éš”ï¼Œé€‚åº”æ—¶åºæ•°æ®
            
            # å®šä¹‰æ—¶åºé›†åˆåç§°
            self.COL_GIFT = "live_gifts"
            self.COL_CHAT = "live_chats"
            self.COL_STATS = "live_stats"          # ã€æ–°å¢ã€‘ç»Ÿè®¡æ•°æ®é›†åˆå           
            logger.info(f"âœ… [Async] MongoDB Client åˆå§‹åŒ–å®Œæˆ: {db_name}")
        except Exception as e:
            logger.error(f"âŒ MongoDB åˆå§‹åŒ–å¤±è´¥: {e}")
            raise e

    async def init_indexes(self):
        """
        åˆå§‹åŒ–ç´¢å¼•åŠ Time Series é›†åˆ
        """
        try:
            existing_cols = await self.db.list_collection_names()

            # --- 1. åˆ›å»ºç¤¼ç‰©æ—¶åºé›†åˆ ---
            if self.COL_GIFT not in existing_cols:
                try:
                    await self.db.create_collection(
                        self.COL_GIFT,
                        timeseries={
                            "timeField": "created_at",   # å¿…é¡»æ˜¯ Date ç±»å‹
                            "metaField": "web_rid",      # ç”¨äºç´¢å¼•å’Œåˆ†æ¡¶çš„å…³é”®å­—æ®µ
                            "granularity": "seconds"     # ç›´æ’­æ•°æ®ç²’åº¦ä¸ºç§’çº§
                        }
                    )
                    logger.info(f"âœ… åˆ›å»ºæ—¶åºé›†åˆ: {self.COL_GIFT}")
                except CollectionInvalid:
                    pass # å¯èƒ½å¹¶å‘åˆ›å»ºå·²å­˜åœ¨

            # --- 2. åˆ›å»ºå¼¹å¹•æ—¶åºé›†åˆ ---
            if self.COL_CHAT not in existing_cols:
                try:
                    await self.db.create_collection(
                        self.COL_CHAT,
                        timeseries={
                            "timeField": "created_at",
                            "metaField": "web_rid",
                            "granularity": "seconds"
                        }
                    )
                    logger.info(f"âœ… åˆ›å»ºæ—¶åºé›†åˆ: {self.COL_CHAT}")
                except CollectionInvalid:
                    pass
            if self.COL_STATS not in existing_cols:
                try:
                    await self.db.create_collection(
                        self.COL_STATS,
                        timeseries={
                            "timeField": "created_at",   # æ—¶é—´å­—æ®µ
                            "metaField": "room_id",      # æ ¸å¿ƒç´¢å¼•å­—æ®µ (æŒ‰æˆ¿é—´èšåˆ)
                            "granularity": "seconds"     # ç²’åº¦
                        }
                    )
                    logger.info(f"âœ… åˆ›å»ºæ—¶åºé›†åˆ: {self.COL_STATS}")
                except CollectionInvalid:
                    pass
            # --- 3. åˆ›å»ºå¸¸è§„ç´¢å¼• ---
            # Authors ç´¢å¼•
            await self.db['authors'].create_index("sec_uid", unique=True)
            
            # Rooms ç´¢å¼•
            await self.db['rooms'].create_index([("room_id", ASCENDING)])
            await self.db['rooms'].create_index([("live_status", ASCENDING)])
            
            # PK å†å²ç´¢å¼•
            await self.db['pk_history'].create_index([("battle_id", ASCENDING), ("room_id", ASCENDING)])
            
            # ä¸ºæ—¶åºé›†åˆè¡¥å……äºŒçº§ç´¢å¼•ï¼ˆmetaField ä¼šè‡ªåŠ¨ç´¢å¼•ï¼Œè¿™é‡ŒåŠ ä¸€äº›å¸¸ç”¨çš„è¿‡æ»¤å­—æ®µï¼‰
            # æ³¨æ„ï¼šMongoDB 5.0+ æ—¶åºé›†åˆç´¢å¼•é™åˆ¶è¾ƒå¤šï¼Œä½† 6.0+ æ”¯æŒæ›´å¤š
            await self.db[self.COL_GIFT].create_index([("gift_name", ASCENDING)])
            await self.db[self.COL_CHAT].create_index([("user_id", ASCENDING)])
            await self.db[self.COL_STATS].create_index([("room_id", ASCENDING), ("created_at", DESCENDING)])
            await self.db[self.COL_GIFT].create_index([("room_id", ASCENDING), ("total_diamond_count", DESCENDING)])
            await self.db[self.COL_GIFT].create_index([("room_id", ASCENDING), ("gift_name", ASCENDING)])
            await self.db[self.COL_GIFT].create_index([("room_id", ASCENDING), ("user_name", ASCENDING)])

# å¼¹å¹•ï¼šæŒ‰æˆ¿é—´+å†…å®¹æœç´¢ï¼ŒæŒ‰æˆ¿é—´+ç”¨æˆ·æœç´¢
            #await self.db[self.COL_CHAT].create_index([("room_id", ASCENDING), ("content", "text")]) # æ–‡æœ¬ç´¢å¼•
            await self.db[self.COL_CHAT].create_index([("room_id", ASCENDING), ("created_at", DESCENDING)])
            await self.db[self.COL_CHAT].create_index([("room_id", ASCENDING), ("user_name", ASCENDING)])
            await self.db[self.COL_CHAT].create_index([("user_name", ASCENDING)])
            await self.db['pk_history'].create_index([("room_id", ASCENDING), ("created_at", DESCENDING)])
            await self.db[self.COL_CHAT].create_index([("sec_uid", ASCENDING)]) # ç”¨äºç²¾å‡†æœID
            logger.info("âœ… æ•°æ®åº“é›†åˆä¸ç´¢å¼•æ£€æŸ¥å®Œæˆ")
        except Exception as e:
            logger.error(f"âŒ ç´¢å¼•/é›†åˆåˆå§‹åŒ–å¤±è´¥: {e}")

    async def save_room_info(self, data: dict):
        """ä¿å­˜ç›´æ’­é—´åŸºç¡€ä¿¡æ¯ (å¸¸è§„é›†åˆ)"""
        if not data: return
        try:
            update_fields = data.copy()
            
            # ã€ä¿®å¤ã€‘å¿…é¡»ä» update_fields ä¸­ç§»é™¤ created_at
            # é˜²æ­¢ä¸ $setOnInsert ä¸­çš„ created_at å†²çª
            if 'created_at' in update_fields:
                update_fields.pop('created_at')

            update_fields['updated_at'] = datetime.now()
            insert_fields = {"created_at": datetime.now()}

            if 'start_follower_count' in update_fields:
                insert_fields['start_follower_count'] = update_fields.pop('start_follower_count')
            else:
                # å¦‚æœ update_fields é‡Œæ²¡æœ‰ï¼Œå¯èƒ½æ˜¯åç»­æ›´æ–°ï¼Œè¿™é‡Œè®¾ä¸ªé»˜è®¤å€¼æˆ–ä¸å¤„ç†
                # ä½†ä¸ºäº† $setOnInsert ä¸æŠ¥é”™ï¼Œä¿æŒåŸé€»è¾‘å³å¯ï¼Œæˆ–è€…ä»…åœ¨ data ä¸­æœ‰æ—¶å¤„ç†
                # è¿™é‡Œä¸ºäº†å®‰å…¨ï¼Œå¯ä»¥ç»™ä¸ªé»˜è®¤å€¼ 0
                insert_fields['start_follower_count'] = 0

            await self.db['rooms'].update_one(
                {"room_id": data['room_id']}, 
                {
                    "$set": update_fields,
                    "$setOnInsert": insert_fields
                },
                upsert=True
            )
        except PyMongoError as e:
            logger.error(f"âŒ [DB] ä¿å­˜ç›´æ’­é—´ä¿¡æ¯å¤±è´¥: {e}")

    async def set_room_ended(self, room_id: str):
        if not room_id: return
        try:
            end_time = datetime.now()
            await self.db['rooms'].update_one(
                {"room_id": room_id},
                {
                    "$set": {
                        "live_status": 4, 
                        "room_status": 4,
                        "end_time": end_time,
                        "updated_at": end_time
                    }
                }
            )
            logger.info(f"ğŸ [DB] ç›´æ’­é—´ {room_id} å·²æ ‡è®°ä¸ºç»“æŸ")
        except PyMongoError as e:
            logger.error(f"âŒ [DB] æ ‡è®°ç»“æŸå¤±è´¥: {e}")

    async def update_room_realtime(self, room_id: str, live_status: int, current_follower_count: int):
        if not room_id: return
        try:
            update_fields = {
                "updated_at": datetime.now(),
                "live_status": live_status, 
                "room_status": live_status, 
            }
            if current_follower_count > 0:
                update_fields["current_follower_count"] = current_follower_count
                room = await self.db['rooms'].find_one({"room_id": room_id}, {"start_follower_count": 1})
                if room:
                    start_count = room.get('start_follower_count', 0)
                    if start_count > 0:
                        update_fields["follower_diff"] = current_follower_count - start_count

            await self.db['rooms'].update_one({"room_id": room_id}, {"$set": update_fields})
        except PyMongoError as e:
            logger.error(f"âŒ [DB] æ›´æ–°å®æ—¶æ•°æ®å¤±è´¥: {e}")

    async def save_author_card(self, data: dict):
        if not data or not data.get('sec_uid'): return
        try:
            data['updated_at'] = datetime.now()
            await self.db['authors'].update_one(
                {"sec_uid": data['sec_uid']}, 
                {"$set": data},
                upsert=True
            )
        except PyMongoError as e:
            logger.error(f"âŒ [DB] ä¿å­˜ä¸»æ’­èµ„æ–™å¤±è´¥: {e}")

    # --------------------------------------------------------------------------
    # é’ˆå¯¹ Time Series ä¼˜åŒ–çš„å†™å…¥é€»è¾‘
    # --------------------------------------------------------------------------

    async def insert_gift(self, data: dict):
        """
        å¼‚æ­¥ä¿å­˜ç¤¼ç‰©ä¿¡æ¯ (Redis ç¼“å†² + æ‰¹é‡å†™å…¥æ—¶åºé›†åˆ)
        """
        if not data: return
        try:
            # ç¡®ä¿ timeField å­˜åœ¨ä¸”ä¸º datetime å¯¹è±¡
            if isinstance(data.get('created_at'), str):
                try:
                    data['created_at'] = datetime.now() 
                except:
                    data['created_at'] = datetime.now()
            elif not data.get('created_at'):
                data['created_at'] = datetime.now()
            
            # åºåˆ—åŒ–å¹¶æ¨å…¥ Redis List
            redis_client = get_redis()
            json_data = json.dumps(data, default=datetime_serializer)
            await redis_client.rpush(self.REDIS_GIFT_KEY, json_data)
            
            # æ£€æŸ¥æ˜¯å¦éœ€è¦æ‰¹é‡å†™å…¥
            current_time = time.time()
            buffer_size = await redis_client.llen(self.REDIS_GIFT_KEY)
            
            if buffer_size >= self.BATCH_SIZE or (current_time - self.LAST_WRITE_TIME > self.BUFFER_TIMEOUT):
                await self.flush_gift_buffer()

        except Exception as e:
            logger.error(f"âŒ [DB] ç¼“å†²ç¤¼ç‰©å¤±è´¥: {e}")

# db.py -> AsyncMongoDBHandler

    async def flush_gift_buffer(self):
        """åˆ·æ–°ç¤¼ç‰©ç¼“å†²åŒº -> live_gifts (TimeSeries) [å®‰å…¨ç‰ˆ]"""
        try:
            redis_client = get_redis()
            # 1. é™åˆ¶æ‰¹æ¬¡å¤§å°
            BATCH_COUNT = 1000
            
            # ä½¿ç”¨ lpop count è·å–æ•°æ® (æˆ–è€… range + trim)
            raw_data_list = await redis_client.lpop(self.REDIS_GIFT_KEY, count=BATCH_COUNT)
            
            if not raw_data_list:
                return

            # ååºåˆ—åŒ–
            current_batch = []
            for raw in raw_data_list:
                try:
                    data = json.loads(raw)
                    data = datetime_deserializer(data)
                    current_batch.append(data)
                except: pass
            
            if not current_batch:
                return
            
            # --- ã€å…³é”®ã€‘å®‰å…¨å†™å…¥é€»è¾‘ ---
            try:
                # 2. å…ˆå†™å…¥è¯¦ç»†è®°å½• (live_gifts)
                await self.db[self.COL_GIFT].insert_many(current_batch, ordered=False)
                
                # ====================================================
                # 3. ã€è¡¥å›ã€‘èšåˆç»Ÿè®¡é€»è¾‘ (æ›´æ–° rooms è¡¨çš„æ€»é’»çŸ³æ•°)
                # ====================================================
                room_diamond_sum = {}
                for gift in current_batch:
                    room_id = gift.get('room_id')
                    diamond = gift.get('total_diamond_count', 0)
                    
                    # å…œåº•è®¡ç®—é€»è¾‘
                    if diamond == 0:
                        d = gift.get('diamond_count', 0)
                        c = gift.get('combo_count', 1)
                        g = gift.get('group_count', 1)
                        diamond = d * c * g
    
                    if room_id and diamond > 0:
                        room_diamond_sum[room_id] = room_diamond_sum.get(room_id, 0) + diamond
                
                
                # æ‰¹é‡æ›´æ–° rooms è¡¨
                for room_id, diamond_inc in room_diamond_sum.items():
                    # åªæœ‰ room_id æ˜¯å­—ç¬¦ä¸²æ—¶æ‰èƒ½åŒ¹é…åˆ°æˆ‘ä»¬åœ¨ main/liveMan ç»Ÿä¸€è¿‡çš„ id
                    await self.db['rooms'].update_one(
                        {"room_id": str(room_id)}, 
                        {
                            "$inc": {"total_diamond_count": diamond_inc},
                            "$set": {"updated_at": datetime.now()}
                        },
                        upsert=True
                    )
                # ====================================================

            except Exception as e:
                logger.error(f"âŒ [DB] æ‰¹é‡å†™å…¥ç¤¼ç‰©å¤±è´¥: {e}")
                # 4. ã€å›æ»šã€‘å¦‚æœå†™å…¥ MongoDB å¤±è´¥ï¼ŒæŠŠæ•°æ®å¡å› Redis é˜²æ­¢ä¸¢å¤±
                if raw_data_list:
                     await redis_client.rpush(self.REDIS_GIFT_KEY, *raw_data_list)

        except Exception as e:
            logger.error(f"âŒ [DB] åˆ·æ–°ç¤¼ç‰©å¼‚å¸¸: {e}")

    async def insert_chat(self, data: dict):
        """
        å¼‚æ­¥ä¿å­˜å¼¹å¹•ä¿¡æ¯ (Redis ç¼“å†² + æ‰¹é‡å†™å…¥æ—¶åºé›†åˆ)
        """
        if not data: return
        try:
            if isinstance(data.get('created_at'), str) or not data.get('created_at'):
                data['created_at'] = datetime.now()

            # åºåˆ—åŒ–å¹¶æ¨å…¥ Redis List
            redis_client = get_redis()
            json_data = json.dumps(data, default=datetime_serializer)
            await redis_client.rpush(self.REDIS_CHAT_KEY, json_data)
            
            # æ£€æŸ¥æ˜¯å¦éœ€è¦æ‰¹é‡å†™å…¥
            current_time = time.time()
            buffer_size = await redis_client.llen(self.REDIS_CHAT_KEY)
            
            if buffer_size >= self.BATCH_SIZE or (current_time - self.LAST_WRITE_TIME > self.BUFFER_TIMEOUT):
                await self.flush_chat_buffer()
        except Exception as e:
            logger.error(f"âŒ [DB] ç¼“å†²å¼¹å¹•å¤±è´¥: {e}")

    async def flush_chat_buffer(self):
        """åˆ·æ–°å¼¹å¹•ç¼“å†²åŒº -> live_chats (TimeSeries) [ä½¿ç”¨ Redis]"""
        try:
            redis_client = get_redis()
            buffer_size = await redis_client.llen(self.REDIS_CHAT_KEY)
            if buffer_size == 0:
                return
        except RuntimeError as e:
            logger.warning(f"âš ï¸ [DB] Redis ä¸å¯ç”¨ï¼Œè·³è¿‡å¼¹å¹•ç¼“å†²åˆ·æ–°: {e}")
            return
        except Exception as e:
            logger.error(f"âŒ [DB] æ£€æŸ¥ Redis ç¼“å†²åŒºå¤±è´¥: {e}")
            return
        
        self.LAST_WRITE_TIME = time.time()

        try:
            # åŸå­æ“ä½œï¼šè¯»å–å¹¶æ¸…ç©º Redis List
            pipe = redis_client.pipeline()
            pipe.lrange(self.REDIS_CHAT_KEY, 0, -1)  # è¯»å–å…¨éƒ¨
            pipe.delete(self.REDIS_CHAT_KEY)         # æ¸…ç©ºåˆ—è¡¨
            results = await pipe.execute()
            
            raw_data_list = results[0]
            if not raw_data_list:
                return
            
            # ååºåˆ—åŒ– JSON æ•°æ®
            current_batch = []
            for raw in raw_data_list:
                try:
                    data = json.loads(raw)
                    data = datetime_deserializer(data)
                    current_batch.append(data)
                except json.JSONDecodeError as e:
                    logger.error(f"âŒ [DB] JSON è§£æå¤±è´¥: {e}")
            
            if not current_batch:
                return
            
            await self.db[self.COL_CHAT].insert_many(current_batch, ordered=False)
            
            # --- èšåˆå¼¹å¹•æ•°é‡ï¼Œæ›´æ–°åˆ° rooms è¡¨ ---
            room_chat_count = {}
            for chat in current_batch:
                room_id = chat.get('room_id')
                if room_id:
                    room_chat_count[room_id] = room_chat_count.get(room_id, 0) + 1
            
            # æ‰¹é‡æ›´æ–° rooms è¡¨
            for room_id, chat_inc in room_chat_count.items():
                await self.db['rooms'].update_one(
                    {"room_id": room_id},
                    {
                        "$inc": {"total_chat_count": chat_inc},
                        "$set": {"updated_at": datetime.now()}
                    },
                    upsert=True
                )
            
            logger.debug(f"ğŸ“¦ [DB] å·²å†™å…¥ {len(current_batch)} æ¡å¼¹å¹•è®°å½•")
                
        except Exception as e:
            logger.error(f"âŒ [DB] åˆ·æ–°å¼¹å¹•å¼‚å¸¸: {e}")
    async def insert_live_stat(self, data: dict):
        """
        ã€æ–°å¢ã€‘å¼‚æ­¥ä¿å­˜ç›´æ’­é—´ç»Ÿè®¡å¿«ç…§ (Redis ç¼“å†²)
        """
        if not data: return
        try:
            # ç¡®ä¿æ—¶é—´å­—æ®µå­˜åœ¨
            if isinstance(data.get('created_at'), str) or not data.get('created_at'):
                data['created_at'] = datetime.now()

            # åºåˆ—åŒ–å¹¶æ¨å…¥ Redis List
            redis_client = get_redis()
            json_data = json.dumps(data, default=datetime_serializer)
            await redis_client.rpush(self.REDIS_STATS_KEY, json_data)
            
            # æ£€æŸ¥æ˜¯å¦éœ€è¦æ‰¹é‡å†™å…¥
            # ç»Ÿè®¡æ•°æ®æ¯5ç§’æ‰æœ‰ä¸€æ¡ï¼Œé¢‘ç‡æ¯”å¼¹å¹•ä½ï¼Œå¯ä»¥é€‚å½“æ”¾å®½ Buffer æ£€æŸ¥
            buffer_size = await redis_client.llen(self.REDIS_STATS_KEY)
            
            # è¿™é‡Œçš„ BATCH_SIZE å¯ä»¥å¤ç”¨ç±»çš„é…ç½®ï¼Œæˆ–è€…è®¾å°ä¸€ç‚¹
            if buffer_size >= 100 or (time.time() - self.LAST_WRITE_TIME > self.BUFFER_TIMEOUT):
                await self.flush_stat_buffer()
        except Exception as e:
            logger.error(f"âŒ [DB] ç¼“å†²ç»Ÿè®¡æ•°æ®å¤±è´¥: {e}")
    # --------------------------------------------------------------------------
# db.py -> AsyncMongoDBHandler

    async def flush_stat_buffer(self):
        """ã€æ–°å¢ã€‘åˆ·æ–°ç»Ÿè®¡ç¼“å†²åŒº -> live_stats"""
        try:
            redis_client = get_redis()
            
            # 1. æ¯æ¬¡å¤„ç† 500 æ¡ï¼Œé˜²æ­¢å†…å­˜çˆ†ç‚¸
            BATCH_COUNT = 500
            
            # ä½¿ç”¨ lpop count (Redis 6.2+) è·å–æ•°æ®
            raw_data_list = await redis_client.lpop(self.REDIS_STATS_KEY, count=BATCH_COUNT)
            
            if not raw_data_list:
                return

            # 2. ååºåˆ—åŒ–
            current_batch = []
            for raw in raw_data_list:
                try:
                    data = json.loads(raw)
                    data = datetime_deserializer(data)
                    current_batch.append(data)
                except: pass
            
            if not current_batch: 
                return

            # 3. å†™å…¥ MongoDB
            try:
                await self.db[self.COL_STATS].insert_many(current_batch, ordered=False)
                # logger.debug(f"ğŸ“ˆ [DB] å·²å†™å…¥ {len(current_batch)} æ¡ç»Ÿè®¡è®°å½•")
            except Exception as e:
                logger.error(f"âŒ [DB] å†™å…¥ç»Ÿè®¡æ•°æ®å¤±è´¥: {e}")
                # 4. ã€å›æ»šã€‘å†™å…¥å¤±è´¥ï¼Œå°†æ•°æ®å¡å› Redis å¤´éƒ¨ï¼Œç­‰å¾…ä¸‹æ¬¡é‡è¯•
                if raw_data_list:
                     await redis_client.lpush(self.REDIS_STATS_KEY, *raw_data_list)

        except Exception as e:
            logger.error(f"âŒ [DB] åˆ·æ–°ç»Ÿè®¡bufferå¼‚å¸¸: {e}")
    async def update_room_stats(self, room_id, stats: dict):
        if not room_id or not stats: return
        try:
            update_fields = {"updated_at": datetime.now()}
            
            if 'user_count' in stats: update_fields['user_count'] = stats['user_count']
            if 'total_user' in stats: update_fields['total_user_count'] = stats['total_user']
            if 'like_count' in stats: update_fields['like_count'] = stats['like_count']
            if 'live_status' in stats: 
                update_fields['live_status'] = stats['live_status']
                update_fields['room_status'] = stats['live_status']
            if 'ranks' in stats: 
                update_fields['ranks'] = stats['ranks']

            pipeline = {"$set": update_fields}
            if 'user_count' in stats:
                pipeline["$max"] = {"max_viewers": stats['user_count']}
                
            await self.db['rooms'].update_one({"room_id": room_id}, pipeline, upsert=True)
        except PyMongoError:
            pass

    async def save_pk_result(self, pk_data: dict):
        if not pk_data: return
        try:
            await self.db['pk_history'].update_one(
                {
                    "battle_id": pk_data['battle_id'],
                    "room_id": pk_data['room_id']
                },
                {"$set": pk_data},
                upsert=True
            )
            logger.info(f"âš”ï¸ [DB] PKæ•°æ®å·²ä¿å­˜: {pk_data['battle_id']}")
        except PyMongoError as e:
            logger.error(f"âŒ [DB] ä¿å­˜PKæ•°æ®å¤±è´¥: {e}")

    async def increment_room_stats(self, room_id: str, inc_data: dict):
        if not room_id or not inc_data: return
        try:
            await self.db['rooms'].update_one(
                {"room_id": room_id},
                {
                    "$inc": inc_data,
                    "$set": {"updated_at": datetime.now()}
                },
                upsert=True
            )
        except Exception as e:
            logger.error(f"âŒ [DB] é€’å¢ç»Ÿè®¡å¤±è´¥: {e}")

# db.py -> AsyncMongoDBHandler -> close

    async def close(self):
        logger.info("ğŸ’¾ æ­£åœ¨å°† Redis ç¼“å†²åŒºæ•°æ®å†™å…¥ MongoDB...")
        await self.flush_chat_buffer()
        await self.flush_gift_buffer()
        await self.flush_stat_buffer()  # ã€æ–°å¢ã€‘åˆ·æ–°ç»Ÿè®¡æ•°æ®
        self.client.close()
        logger.info("ğŸ‘‹ MongoDB è¿æ¥å·²å…³é—­")

    async def get_room_live_status(self, room_id: str):
        """
        ã€æ–°å¢ã€‘è·å–æŒ‡å®šæˆ¿é—´çš„å½“å‰æ•°æ®åº“çŠ¶æ€
        ç”¨äº main.py åˆ¤æ–­æ˜¯å¦éœ€è¦é‡å¯å½•åˆ¶
        """
        try:
            res = await self.db['rooms'].find_one(
                {"room_id": room_id}, 
                {"live_status": 1}
            )
            if res:
                return res.get('live_status', 0)
        except Exception:
            pass
        return 0
    async def get_all_cookies(self):
        """è·å–æ‰€æœ‰ Cookie"""
        cookies = []
        # æ³¨æ„ï¼šè¿™é‡Œè¦å¤„ç†ä¸€ä¸‹åˆ¤ç©ºï¼Œé˜²æ­¢è¿”å› [None]
        async for doc in self.db['settings_cookies'].find({}, {"_id": 0}):
            if doc.get('cookie'):
                cookies.append(doc['cookie'])
        return cookies

    async def add_cookie(self, cookie_str: str):
        """æ·»åŠ ä¸€ä¸ª Cookie"""
        if not cookie_str: return
        # ä½¿ç”¨ $addToSet ä¿è¯ä¸é‡å¤æ·»åŠ 
        await self.db['sys_config'].update_one(
            {"key": "douyin_cookies"},
            {"$addToSet": {"cookies": cookie_str}},
            upsert=True
        )

    async def delete_cookie(self, cookie_str: str):
        """åˆ é™¤å¤±æ•ˆ Cookie"""
        if not cookie_str: return
        await self.db['settings_cookies'].delete_one({"cookie": cookie_str})
        logger.info(f"ğŸ—‘ï¸ [DB] å·²ç§»é™¤å¤±æ•ˆ Cookie: {cookie_str[:20]}...")
        
        
        